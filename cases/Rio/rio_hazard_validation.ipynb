{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rio de Janeiro hazard validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents a workflow for validating the Sfincs flood hazard maps of the Acari River Basin using **HydroFlows**, based on the floodmarks survey conducted for the January 2024 event. The folder structure is the same as in the other notebooks (the user is reffered to the `rio_risk_climate_strategies` notebook). Precipitation is applied uniformly across the Acari using values from station Iraja (ID 11) for the historical event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pathlib import Path\n",
    "\n",
    "from hydroflows import Workflow, WorkflowConfig\n",
    "from hydroflows.log import setuplog\n",
    "from hydroflows.methods import (\n",
    "    catalog,\n",
    "    hazard_validation,\n",
    "    historical_events,\n",
    "    script,\n",
    "    sfincs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define case name, root directory and logger\n",
    "\n",
    "pwd = Path().resolve() # current directory\n",
    "name = \"local-hazard-validation\"  # case name\n",
    "setup_root = Path(pwd, \"setups\", \"local\")\n",
    "pwd_rel = \"../../\"  # relative path from the case directory to the current file\n",
    "\n",
    "# Setup the logger\n",
    "logger = setuplog(level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the workflow\n",
    "\n",
    "In this block the workflow configuration is specified and a HydroFlows workflow is created. The workflow takes as input the following:\n",
    "- the region polygon of the Acari river basin\n",
    "- the data catalog files describing all the input datasets\n",
    "- the HydroMT configuration files\n",
    "- the model executables\n",
    "\n",
    "In addition some more general settings are specified (like the historical event name and start/end time) that are used in the methods below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the config file and data libs\n",
    "\n",
    "# Config\n",
    "config = WorkflowConfig(\n",
    "    # general settings\n",
    "    region=Path(pwd, \"data/region.geojson\"),\n",
    "    catalog_path_global=Path(pwd, \"data/global-data/data_catalog.yml\"),\n",
    "    catalog_path_local=Path(pwd, \"data/local-data/data_catalog.yml\"),\n",
    "    plot_fig=True,\n",
    "    # sfincs settings\n",
    "    hydromt_sfincs_config=Path(setup_root, \"hydromt_config/sfincs_config_default.yml\"),\n",
    "    sfincs_exe=Path(pwd, \"bin/sfincs_v2.1.1/sfincs.exe\"),\n",
    "    depth_min=0.05,\n",
    "    subgrid_output=True,  # sfincs subgrid output should exist since it is used to downscale the floodmap\n",
    "    # historical events settings\n",
    "    historical_events_dates={\n",
    "        \"event_january_2024\": {\n",
    "            \"startdate\": \"2024-01-12 23:00\",\n",
    "            \"enddate\": \"2024-01-14 21:00\",\n",
    "        },\n",
    "    },\n",
    "    # validation settings\n",
    "    floodmarks_geom=Path(pwd, \"data/local-data/floodmap/laminas_levantadas.gpkg\"),\n",
    "    waterlevel_col=\"altura_cm\",\n",
    "    waterlevel_unit=\"cm\",\n",
    "    cmap=\"PiYG\",\n",
    "    bins=[-2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5],\n",
    "    bmap=\"CartoDB.PositronNoLabels\",\n",
    ")\n",
    "\n",
    "w = Workflow(\n",
    "    config=config,\n",
    "    name=name,\n",
    "    root=setup_root,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the global and local data catalogs\n",
    "\n",
    "Both local and global information is needed to build the SFINCS model. For this reason, with the following method we merge the two data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge global and local data catalogs\n",
    "merged_catalog_global_local = catalog.MergeCatalogs(\n",
    "    catalog_path1=w.get_ref(\"$config.catalog_path_global\"),\n",
    "    catalog_path2=w.get_ref(\"$config.catalog_path_local\"),\n",
    "    merged_catalog_path=Path(pwd, \"data/merged_data_catalog_local_global.yml\"),\n",
    ")\n",
    "w.create_rule(merged_catalog_global_local, rule_id=\"merge_global_local_catalogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build SFINCS\n",
    "\n",
    "The following method builds the SFINCS model for the Acari River Basin. It takes as input the region geometry, the HydroMT configuration file and the merged global-local data catalog and it generates SFINCS models saved in the models dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SFINCS model for the Acari river basin\n",
    "# - settings from the hydromt_config per strategy\n",
    "# - data from the merged global-local catalog\n",
    "\n",
    "# Sfincs build\n",
    "sfincs_build = sfincs.SfincsBuild(\n",
    "    region=w.get_ref(\"$config.region\"),\n",
    "    sfincs_root=\"models/sfincs_default\",\n",
    "    config=w.get_ref(\"$config.hydromt_sfincs_config\"),\n",
    "    catalog_path=merged_catalog_global_local.output.merged_catalog_path,\n",
    "    plot_fig=w.get_ref(\"$config.plot_fig\"),\n",
    "    subgrid_output=w.get_ref(\"$config.subgrid_output\"),\n",
    ")\n",
    "w.create_rule(sfincs_build, rule_id=\"sfincs_build\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precipitation design events\n",
    "\n",
    "Here, we preprocess the local precipitation file using the `preprocess_local_precip.py` script. This step generates a NetCDF file compatible with the HydroFlows `HistoricalEvents` method, which is used to derive the historical event, i.e. January's 2024, used for validation. The hisotrical event(s) is then saved in the event/historical directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess local precipitation data and get the historical event for validation, i.e January's 2024\n",
    "# Preprocess precipitation\n",
    "precipitation = script.ScriptMethod(\n",
    "    script=Path(pwd, \"scripts\", \"preprocess_local_precip.py\"),\n",
    "    output={\n",
    "        \"precip_nc\": Path(\n",
    "            pwd, \"data/preprocessed-data/output_scalar_resampled_precip_station11.nc\"\n",
    "        )\n",
    "    },\n",
    ")\n",
    "w.create_rule(precipitation, rule_id=\"preprocess_local_rainfall\")\n",
    "\n",
    "historical_event = historical_events.HistoricalEvents(\n",
    "    precip_nc=precipitation.output.precip_nc,\n",
    "    events_dates=w.get_ref(\"$config.historical_events_dates\"),\n",
    "    output_dir=\"events/historical\",\n",
    "    wildcard=\"pluvial_historical_event\",\n",
    ")\n",
    "w.create_rule(historical_event, rule_id=\"historical_event\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive flood hazard\n",
    "\n",
    "In the following block, we derive the flood hazard for the historical event. The hazard derivation is performed using the `SfincsUpdateForcing`, `SfincsRun`, and `SfincsDownscale` methods. First, the SFINCS historical forcing is updated using `SfincsUpdateForcing`. Then, the model execution is carried out using the `SfincsRun` method and finally the `SfincsDownscale` method is used to generate high-resolution flood hazard maps from the SFINCS output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the SFINCS model with the historical event\n",
    "# This will create new SFINCS instances for the hisotrical event in the simulations subfolder\n",
    "sfincs_update = sfincs.SfincsUpdateForcing(\n",
    "    sfincs_inp=sfincs_build.output.sfincs_inp,\n",
    "    event_yaml=historical_event.output.event_yaml,\n",
    "    output_dir=sfincs_build.output.sfincs_inp.parent\n",
    "    / \"simulations\"\n",
    "    / \"{pluvial_historical_event}\",\n",
    ")\n",
    "w.create_rule(sfincs_update, rule_id=\"sfincs_update\")\n",
    "\n",
    "# Run the SFINCS model for the historical event\n",
    "# This will create simulated water levels for the historical event\n",
    "sfincs_run = sfincs.SfincsRun(\n",
    "    sfincs_inp=sfincs_update.output.sfincs_out_inp,\n",
    "    sfincs_exe=w.get_ref(\"$config.sfincs_exe\"),\n",
    ")\n",
    "w.create_rule(sfincs_run, rule_id=\"sfincs_run\")\n",
    "\n",
    "# Downscale the SFINCS output to derive high-res flood hazard maps\n",
    "sfincs_down = sfincs.SfincsDownscale(\n",
    "    sfincs_map=sfincs_run.output.sfincs_map,\n",
    "    sfincs_subgrid_dep=sfincs_build.output.sfincs_subgrid_dep,\n",
    "    depth_min=w.get_ref(\"$config.depth_min\"),\n",
    "    output_root=\"output/hazard_historical\",\n",
    ")\n",
    "w.create_rule(sfincs_down, rule_id=\"sfincs_downscale\")\n",
    "\n",
    "\n",
    "# the simulation is stored in:\n",
    "print(\"simulation folder:\", sfincs_update.output.sfincs_out_inp.parent)\n",
    "\n",
    "# the hazard map is stored in:\n",
    "print(\"high-res hazard map folder:\", sfincs_down.output.hazard_tif.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hazard validation\n",
    "\n",
    "In this step we validate the derived downscaled inundation map for the historical event against floodmarks. This is done by using the `FloodmarksValidation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the downscaled inundation map against floodmarks\n",
    "floodmark_validation = hazard_validation.FloodmarksValidation(\n",
    "    floodmarks_geom=w.get_ref(\"$config.floodmarks_geom\"),\n",
    "    flood_hazard_map=sfincs_down.output.hazard_tif,\n",
    "    waterlevel_col=w.get_ref(\"$config.waterlevel_col\"),\n",
    "    waterlevel_unit=w.get_ref(\"$config.waterlevel_unit\"),\n",
    "    out_root=\"output/validation/{pluvial_historical_event}\",\n",
    "    bmap=w.get_ref(\"$config.bmap\"),\n",
    "    bins=w.get_ref(\"$config.bins\"),\n",
    "    cmap=w.get_ref(\"$config.cmap\"),\n",
    "    region=sfincs_build.output.sfincs_region,\n",
    ")\n",
    "w.create_rule(floodmark_validation, rule_id=\"floodmark_validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and execute the workflow\n",
    "\n",
    "The workflow can be executed using HydroFlows or a workflow engine. Below we first plot and dryrun the workflow to check if it is correctly defined. Then, we parse the workflow to SnakeMake to execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.plot_rulegraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.dryrun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to snakemake\n",
    "w.to_snakemake(f\"{name}.smk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydroflows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
