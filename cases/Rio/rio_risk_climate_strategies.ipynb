{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rio de Janeiro flood risk assessment\n",
    "\n",
    "This notebook presents a workflow for assessing pluvial flood risk for the Acari River Basin under different climate scenarios and adaptation strategies using **HydroFlows**. The workflow integrates local data, incorporating real adaptation measures outlined in the city's new master plan. These measures were identified through discussions with local partners, ensuring that the analysis reflects both existing conditions and planned interventions.  \n",
    "\n",
    "To develop a comprehensive flood risk assessment, we created hazard (SFINCS) and impact (FIAT) models using local datasets, either in their raw form or after preprocessing. Whenever preprocessing was required, those steps were explicitly included in the workflow to maintain full reproducibility. In cases where local data were insufficient, global datasets were used to fill gaps and meet model requirements.  \n",
    "\n",
    "In addition, local precipitation data was used to generate design events for different return periods. These events were then **scaled using climate change (CC) scaling techniques** to simulate future climate conditions. The study considers three climate scenarios:  \n",
    "- The **Present (Historical)** scenario with no temperature change.  \n",
    "- A **Moderate Emissions (RCP4.5, 2050)** scenario with a projected temperature increase of **+1.2°C**.  \n",
    "- A **High Emissions (RCP8.5, 2050)** scenario with a projected temperature increase of **+2.5°C**.  \n",
    "\n",
    "For each of these climate scenarios, we evaluated three adaptation strategies to assess their effectiveness in reducing flood risk:  \n",
    "- A **default (no strategy)**, representing the current state of the system.  \n",
    "- A **reservoir-based strategy**, accounting for newly planned reservoirs designed to buffer flood volumes.  \n",
    "- A **dredging strategy**, which simulates the impact of sediment removal on flood mitigation.  \n",
    "\n",
    "By systematically analyzing these climate and adaptation scenarios, this study provides a structured and reproducible approach to understanding flood risk dynamics in the Acari basin. The insights gained can support decision-making and inform future urban resilience planning efforts.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pathlib import Path\n",
    "\n",
    "from hydroflows import Workflow, WorkflowConfig\n",
    "from hydroflows.log import setuplog\n",
    "from hydroflows.methods import catalog, fiat, rainfall, script, sfincs\n",
    "from hydroflows.workflow.wildcards import resolve_wildcards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folder Structure  \n",
    "\n",
    "The folder is organized to facilitate reproducibility and clarity in handling data, scripts, and configurations for flood risk assessment. Below is an overview of the key components:  \n",
    "\n",
    "1. Model Executables (`bin/`)  \n",
    "The `bin/` directory stores the executable files required to run the models, namely **SFINCS** and **FIAT**  \n",
    "\n",
    "2. Data Directory (`data/`)  \n",
    "All input and processed data are stored within this directory. It contains the following subfolders:  \n",
    "\n",
    "- `global-data/`: Stores global datasets, with a corresponding `data_catalog.yml` file documenting the required sources and paths.  \n",
    "- `local-data/`: Contains local datasets, also accompanied by a `data_catalog.yml` file for reference.  \n",
    "- `preprocessed-data/`: Stores datasets generated through preprocessing of local or global data. The preprocessing script automatically generates a `data_catalog.yml` file (shown later in the workflow).  \n",
    "\n",
    "3. Scripts Directory (`scripts/`)  \n",
    "This folder contains Python scripts used for data preprocessing and analysis.\n",
    "\n",
    "4. Model Setups (`setups/`)  \n",
    "The `setups/` directory is divided into two subfolders, `global/` and `local/`, representing different modelling configurations for the Acari basin (in this noteboook only local). In the setup directories the built models as well as their output will be saved by case. Each setup contains a `hydromt_config/` folder where configuration files for **HydroMT** are stored.  The `local/` setup includes specific configuration files for the **SFINCS** model, corresponding to different adaptation strategies, and also the configuration file for the **FIAT** model:  \n",
    "  - `sfincs_config_default.yml` (current system state)  \n",
    "  - `sfincs_config_reservoirs.yml` (planned reservoirs)  \n",
    "  - `sfincs_config_dredging.yml` (dredging scenario)  \n",
    "  - `fiat_config.yml`\n",
    "\n",
    "Overview:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Rio\n",
    "├── bin\n",
    "│   ├── fiat_v0.2.1\n",
    "│   └── sfincs_v2.1.1\n",
    "├── data\n",
    "│   ├── global-data\n",
    "│   │   └── data_catalog.yml\n",
    "│   ├── local-data\n",
    "│   │   └── data_catalog.yml\n",
    "│   ├── preprocessed-data\n",
    "│   │   └── data_catalog.yml\n",
    "│   └── region.gpkg\n",
    "├── scripts\n",
    "├── setups\n",
    "│   ├── global\n",
    "│   │   └── hydromt_config\n",
    "│   └── local\n",
    "│       └── hydromt_config\n",
    "│           ├── sfincs_config_default.yml\n",
    "│           ├── sfincs_config_reservoirs.yml\n",
    "│           ├── sfincs_config_dredging.yml\n",
    "│           └── fiat_config.yml\n",
    "└── rio_risk_climate_strategies.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define case name, root directory and logger\n",
    "\n",
    "pwd = Path().resolve() # current directory\n",
    "name = \"local-risk-climate-strategies\"\n",
    "setup_root = Path(pwd, \"setups\", \"local\")\n",
    "pwd_rel = \"../../\"  # relative path from the case directory to the current file\n",
    "\n",
    "# Setup the logger\n",
    "logger = setuplog(level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the workflow\n",
    "\n",
    "In this block the workflow configuration is specified and a HydroFlows workflow is created. The workflow takes as input the following:\n",
    "- the region polygon of the Acari river basin\n",
    "- the data catalog files describing all the input datasets\n",
    "- the HydroMT configuration files\n",
    "- the model executables\n",
    "\n",
    "In addition some more general settings are specified that are used in the methods below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the config file and data libs\n",
    "\n",
    "\n",
    "strategies = [\"default\", \"reservoirs\", \"dredging\"]\n",
    "\n",
    "# Config\n",
    "config = WorkflowConfig(\n",
    "    # general settings\n",
    "    region=Path(pwd_rel, \"data/region.geojson\"),\n",
    "    plot_fig=True,\n",
    "    catalog_path_global=Path(pwd_rel, \"data/global-data/data_catalog.yml\"),\n",
    "    catalog_path_local=Path(pwd_rel, \"data/local-data/data_catalog.yml\"),\n",
    "    # sfincs settings\n",
    "    sfincs_exe=Path(pwd_rel, \"bin/sfincs_v2.1.1/sfincs.exe\"),\n",
    "    depth_min=0.05,\n",
    "    subgrid_output=True,  # sfincs subgrid output should exist since it is used in the fiat model\n",
    "    # fiat settings\n",
    "    hydromt_fiat_config=Path(\"hydromt_config/fiat_config.yml\"),\n",
    "    fiat_exe=Path(pwd_rel, \"bin/fiat_v0.2.1/fiat.exe\"),\n",
    "    risk=True,\n",
    "    # design events settings\n",
    "    rps=[5, 10, 100],\n",
    "    start_date=\"1990-01-01\",\n",
    "    end_date=\"2023-12-31\",\n",
    "    # Climate rainfall scenarios settings (to be applied on the derived design events)\n",
    "    # Dictionary where:\n",
    "    # - Key: Scenario name (e.g., \"current\", \"rcp45\", \"rcp85\")\n",
    "    # - Value: Corresponding temperature delta (dT) for each scenario\n",
    "    scenarios_dict = {\n",
    "        \"present\": 0,  # No temperature change for the present (or historical) scenario\n",
    "        \"rcp45_2050\": 1.2,  # Moderate emissions scenario with +1.2°C\n",
    "        \"rcp85_2050\": 2.5,  # High emissions scenario with +2.5°C\n",
    "    },\n",
    "    # Strategies settings\n",
    "    strategies_dict = {\"strategies\": strategies}\n",
    ")\n",
    "\n",
    "\n",
    "w = Workflow(config=config, name=name, root=setup_root, wildcards=config.strategies_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess local data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess local exposure data\n",
    "In this step, local exposure data (stored in data/local-data) are preprocessed using Python scripts executed via the HydroFlows ScriptMethod. First, the clip_exposure.py script clips the exposure data to the regional boundary. The outputs of this step are saved in the data/pre-processed-data, namely census2010.gpkg, building_footprints.gpkg and entrances.gpkg. These clipped datasets are then further processed using the preprocess_exposure.py script, executed again via ScriptMethod (rule fiat_preprocess_clip_exp). The final output is a new data catalog stored in data/preprocessed-data, including references to the newly generated datasets, needed to build the FIAT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip exposure datasets to the region of interest.\n",
    "fiat_clip_exp = script.ScriptMethod(\n",
    "    script=Path(pwd_rel, \"scripts\", \"clip_exposure.py\"),\n",
    "    # Note that the output paths/names are hardcoded in the scipt\n",
    "    # These names are used in the hydromt_fiat config\n",
    "    input={\n",
    "        \"region\": Path(pwd_rel, \"data/region.geojson\"),\n",
    "    },\n",
    "    output={\n",
    "        \"census\": Path(pwd_rel, \"data/preprocessed-data/census2010.gpkg\"),\n",
    "        \"building_footprints\": Path(\n",
    "            pwd_rel, \"data/preprocessed-data/building_footprints.gpkg\"\n",
    "        ),\n",
    "        \"entrances\": Path(pwd_rel, \"data/preprocessed-data/entrances.gpkg\"),\n",
    "    },\n",
    ")\n",
    "w.create_rule(fiat_clip_exp, rule_id=\"fiat_clip_exposure\")\n",
    "\n",
    "# Preprocess clipped exposure\n",
    "fiat_preprocess_clip_exp = script.ScriptMethod(\n",
    "    script=Path(pwd_rel, \"scripts\", \"preprocess_exposure.py\"),\n",
    "    input={\n",
    "        \"census\": fiat_clip_exp.output.census,\n",
    "        \"building_footprints\": fiat_clip_exp.output.building_footprints,\n",
    "        \"entrances\": fiat_clip_exp.output.entrances,\n",
    "    },\n",
    "    output={\n",
    "        \"preprocessed_data_catalog\": Path(\n",
    "            pwd_rel, \"data/preprocessed-data/data_catalog.yml\"\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "w.create_rule(fiat_preprocess_clip_exp, rule_id=\"fiat_preprocess_exposure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the global and local data catalogs\n",
    "\n",
    "Both local and global information is needed to build the SFINCS model. For this reason, with the following method we merge the two data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge global and local data catalogs\n",
    "merge_all_catalogs = catalog.MergeCatalogs(\n",
    "    catalog_path1=w.get_ref(\"$config.catalog_path_global\"),\n",
    "    catalog_path2=w.get_ref(\"$config.catalog_path_local\"),\n",
    "    catalog_path3=fiat_preprocess_clip_exp.output.preprocessed_data_catalog,\n",
    "    merged_catalog_path=Path(pwd_rel, \"data/merged_data_catalog_all.yml\"),\n",
    ")\n",
    "w.create_rule(merge_all_catalogs, rule_id=\"merge_all_catalogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Hazard and Impact models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build SFINCS\n",
    "\n",
    "The following method builds the SFINCS model for the Acari River Basin per adaptation strategy. It takes as input the region geometry, the HydroMT configuration files per strategy and the merged global-local data catalog and it generates SFINCS models saved in the models dir. The strategy is indicated by the name of the directory, e.g., models/sfincs_reservoirs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SFINCS model for the Acari river basin\n",
    "# - settings from the hydromt_config per strategy\n",
    "# - data from the merged global-local catalog\n",
    "\n",
    "# Note that subgrid_output is set to True, since the subgrid output is used in the fiat model\n",
    "sfincs_build = sfincs.SfincsBuild(\n",
    "    region=w.get_ref(\"$config.region\"),\n",
    "    sfincs_root=\"models/sfincs_{strategies}\",\n",
    "    config=Path(\"hydromt_config/sfincs_config_{strategies}.yml\"),\n",
    "    catalog_path=merge_all_catalogs.output.merged_catalog_path,\n",
    "    plot_fig=w.get_ref(\"$config.plot_fig\"),\n",
    "    subgrid_output=w.get_ref(\"$config.subgrid_output\"),\n",
    ")\n",
    "w.create_rule(sfincs_build, rule_id=\"sfincs_build\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build FIAT\n",
    "\n",
    "The following method builds the FIAT model for the Acari River Basin. It takes as inputs the sfincs_build output for the model region and ground elevation, the HydroMT configuration file and the merged preprocessed-global-local data catalog and it generates a FIAT model saved in the models dir. Note that since one FIAT model is required only inputs from one SFINCS model are taken using `resolve_wildcards` from `hydroflows.workflow.wildcards`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiat build\n",
    "# - the sfincs_build output for the model region and ground elevation; Note we use only the SFINCS model of the first strategy\n",
    "# - settings from the hydromt_config\n",
    "# - data from the merged catalog\n",
    "\n",
    "fiat_build = fiat.FIATBuild(\n",
    "    region=resolve_wildcards(\n",
    "        sfincs_build.output.sfincs_region, {\"strategies\": strategies[0]}\n",
    "    ),\n",
    "    ground_elevation=resolve_wildcards(\n",
    "        sfincs_build.output.sfincs_subgrid_dep, {\"strategies\": strategies[0]}\n",
    "    ),\n",
    "    fiat_root=\"models/fiat_default\",\n",
    "    catalog_path=merge_all_catalogs.output.merged_catalog_path,\n",
    "    config=w.get_ref(\"$config.hydromt_fiat_config\"),\n",
    ")\n",
    "w.create_rule(fiat_build, rule_id=\"fiat_build\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precipitation design events\n",
    "\n",
    "Here, we preprocess the local precipitation file using the preprocess_local_precip.py script. This step generates a NetCDF file compatible with the HydroFlows PluvialDesignEvents method, which is used to derive pluvial design events for various return periods. The design events are then saved in the event/design directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Present climate events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess precipitation\n",
    "precipitation = script.ScriptMethod(\n",
    "    script=Path(pwd_rel, \"scripts\", \"preprocess_local_precip.py\"),\n",
    "    # Note that the output path/filename is hardcoded in the scipt\n",
    "    output={\n",
    "        \"precip_nc\": Path(\n",
    "            pwd_rel, \"data/preprocessed-data/output_scalar_resampled_precip_station11.nc\"\n",
    "        )\n",
    "    },\n",
    ")\n",
    "w.create_rule(precipitation, rule_id=\"preprocess_local_rainfall\")\n",
    "\n",
    "# Derive desing pluvial events from the preprocessed local precipitation\n",
    "pluvial_design_events = rainfall.PluvialDesignEvents(\n",
    "    precip_nc=precipitation.output.precip_nc,\n",
    "    rps=w.get_ref(\"$config.rps\"),\n",
    "    wildcard=\"events\",\n",
    "    event_root=\"events/default\",\n",
    ")\n",
    "w.create_rule(pluvial_design_events, rule_id=\"get_pluvial_design_events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future climate scenarios\n",
    "\n",
    "The pluvial design events generated in the previous step are scaled for the different climate scenarios (temperature changes; see scenarios_dict) using the `FutureClimateRainfall` method. This step produces scaled events, which are saved in the events/{scenario_name} directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Climate rainfall scenarios events\n",
    "scenarios_design_events = rainfall.FutureClimateRainfall(\n",
    "    scenarios=w.get_ref(\"$config.scenarios_dict\"),\n",
    "    event_names=pluvial_design_events.params.event_names,\n",
    "    event_set_yaml=pluvial_design_events.output.event_set_yaml,\n",
    "    event_wildcard=\"events\",  # we overwrite the wildcard\n",
    "    scenario_wildcard=\"scenarios\",\n",
    "    event_root=\"events\",\n",
    ")\n",
    "w.create_rule(scenarios_design_events, rule_id=\"scenarios_pluvial_design_events\")\n",
    "\n",
    "# The produced event set is saved as follows:\n",
    "print(\"Output event sets\", scenarios_design_events.output.future_event_set_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive flood hazard\n",
    "\n",
    "In the following block, we derive the flood hazard for the different events per scenario and strategy. The hazard derivation is performed using the `SfincsUpdateForcing`, `SfincsRun`, `SfincsPostprocess`, and `SfincsDownscale` methods. First, the SFINCS forcing per event is updated using `SfincsUpdateForcing`. Then, the model execution is carried out using the `SfincsRun` method. The total number of models simulations will be equal to return periods × scenarios × strategies. The outputs of these model runs are then postprocessed. The `SfincsPostprocess` method converts SFINCS results into a regular grid of maximum water levels, which is required to update the FIAT model. The `SfincsDownscale` method then refines the SFINCS output to generate high-resolution flood hazard maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the SFINCS model with pluvial events\n",
    "# This will create new SFINCS instances for each fluvial event in the simulations subfolder\n",
    "sfincs_update = sfincs.SfincsUpdateForcing(\n",
    "    sfincs_inp=sfincs_build.output.sfincs_inp,\n",
    "    event_yaml=scenarios_design_events.output.future_event_yaml,\n",
    "    output_dir=sfincs_build.output.sfincs_inp.parent / \"simulations_{scenarios}\" / \"{events}\",\n",
    ")\n",
    "w.create_rule(sfincs_update, rule_id=\"sfincs_update\")\n",
    "\n",
    "# Run the SFINCS model for each fluvial event\n",
    "# This will create simulated water levels for each pluvial event\n",
    "sfincs_run = sfincs.SfincsRun(\n",
    "    sfincs_inp=sfincs_update.output.sfincs_out_inp,\n",
    "    sfincs_exe=w.get_ref(\"$config.sfincs_exe\"),\n",
    ")\n",
    "w.create_rule(sfincs_run, rule_id=\"sfincs_run\")\n",
    "\n",
    "# Postprocesses SFINCS results to a regular grid of maximum water levels\n",
    "sfincs_post = sfincs.SfincsPostprocess(\n",
    "    sfincs_map=sfincs_run.output.sfincs_map,\n",
    ")\n",
    "w.create_rule(sfincs_post, rule_id=\"sfincs_post\")\n",
    "\n",
    "# Downscale the SFINCS output to derive high-res flood hazard maps\n",
    "sfincs_down = sfincs.SfincsDownscale(\n",
    "    sfincs_map=sfincs_run.output.sfincs_map,\n",
    "    sfincs_subgrid_dep=sfincs_build.output.sfincs_subgrid_dep,\n",
    "    depth_min=w.get_ref(\"$config.depth_min\"),\n",
    "    output_root=\"output/hazard_{scenarios}_{strategies}\",\n",
    ")\n",
    "w.create_rule(sfincs_down, rule_id=\"sfincs_downscale\")\n",
    "\n",
    "# the simulations are stored in:\n",
    "print(\"simulation folder:\", sfincs_update.output.sfincs_out_inp.parent)\n",
    "\n",
    "# the hazard maps are stored in:\n",
    "print(\"high-res hazard map folder:\", sfincs_down.output.hazard_tif.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive flood risk\n",
    "\n",
    "In the following block, we derive the flood risk for the different hazards per scenario and strategy produced above. The risk derivation is performed using `FIATUpdateHazard` and `FIATRun`, while then final outcome is visualized with `FIATVisualize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update hazard forcing with the pluvial eventset to compute risk\n",
    "fiat_update = fiat.FIATUpdateHazard(\n",
    "    fiat_cfg=fiat_build.output.fiat_cfg,\n",
    "    event_set_yaml=scenarios_design_events.output.future_event_set_yaml,\n",
    "    map_type=\"water_level\",\n",
    "    hazard_maps=sfincs_post.output.sfincs_zsmax,\n",
    "    risk=w.get_ref(\"$config.risk\"),\n",
    "    output_dir=fiat_build.output.fiat_cfg.parent\n",
    "    / \"simulations_{scenarios}_{strategies}\",\n",
    ")\n",
    "w.create_rule(fiat_update, rule_id=\"fiat_update\")\n",
    "\n",
    "# Run FIAT to compute flood risk\n",
    "fiat_run = fiat.FIATRun(\n",
    "    fiat_cfg=fiat_update.output.fiat_out_cfg,\n",
    "    fiat_exe=w.get_ref(\"$config.fiat_exe\"),\n",
    ")\n",
    "w.create_rule(fiat_run, rule_id=\"fiat_run\")\n",
    "\n",
    "# Visualize FIAT results\n",
    "fiat_visualize_risk = fiat.FIATVisualize(\n",
    "    fiat_output_csv=fiat_run.output.fiat_out_csv,\n",
    "    fiat_cfg=fiat_update.output.fiat_out_cfg,\n",
    "    spatial_joins_cfg=fiat_build.output.spatial_joins_cfg,\n",
    "    output_dir=\"output/risk_{scenarios}_{strategies}\",\n",
    ")\n",
    "w.create_rule(fiat_visualize_risk, rule_id=\"fiat_visualize_risk\")\n",
    "\n",
    "# fiat simulation folders\n",
    "print(\"fiat simulation folder:\", fiat_update.output.fiat_out_cfg.parent)\n",
    "\n",
    "# risk informetrics/infographics are stored\n",
    "print(\"risk informetrics/infographics output folder:\", fiat_visualize_risk.output.fiat_infographics.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup FloodAdapt\n",
    "\n",
    "A FloodAdapt database is created from SFINCS / Delft-FIAT models and the HydroFlows EventSet definition with the SetupFloodAdapt method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# floodadapt_build = flood_adapt.SetupFloodAdapt(\n",
    "#     sfincs_inp=sfincs_build.output.sfincs_inp,\n",
    "#     fiat_cfg=fiat_build.output.fiat_cfg,\n",
    "#     event_set_yaml=pluvial_design_events.output.event_set_yaml,\n",
    "#     output_dir=\"models/flood_adapt_builder_{strategies}\",\n",
    "# )\n",
    "# w.create_rule(floodadapt_build, rule_id=\"floodadapt_build\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and execute the workflow\n",
    "\n",
    "The workflow can be executed using HydroFlows or a workflow engine. Below we first plot and dryrun the workflow to check if it is correctly defined. Then, we parse the workflow to SnakeMake to execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.plot_rulegraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.dryrun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to snakemake\n",
    "w.to_snakemake(f\"{name}.smk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydroflows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
